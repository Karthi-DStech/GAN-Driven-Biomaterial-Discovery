{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "185a8507",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, datasets, models\n",
    "import torchvision\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f6fb67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0d9a432",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b25592",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6cff1088",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = '/Users/karthik/GANS/1by4-TOPO-Resized/bio_materials/processed_Pattern_FeatureIdx_1000_TopoUnit_4X4.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ee90a3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMAAAADFCAAAAAAneqJoAAAEYklEQVR4nO2d2ZLbIBBFIZX//2XlYSYey6L3pheN7kNiV0mCw+0GhCVmHqO3/mRXwKoHIFsPgLPmFJ5QDGCOISSoBTBf/7BVCmCe/uOpEsC8fGDoL//qm8dsafZ+i+mAuHMQa4JfcPEAFMkl1ES/IuIA7G/+a4XZRdIAU5VbMi2uzC2MBJjAZ0etHWYWRvRC+4PHWgbqwKVpdvCA1+QVhjhgqy1x9mtQQY6bnJEHdsASmHyhF+SUBjkQEfx0KQwP1g7APb8rGH0x+oglAHYal4ARv5xLkccsQsinkT1al6OLA+S8wSuImNehDsu6H+DPr4gDFQCsoh2THb/UBcAp+TyFlnd1oN5iKVojVQ7EWoC36ALAwwI/xIOojq4XirOAbM0VQKE8Vs6FzEHkxceoiHYgC7FAfz9Qoiul0vdLwIyb0cDI5T3toSiAECqUx4QqLe6qBAFYLAj1BnSgRB4zZAmhElkAA6gtiOUyJXEFCxCAHl0p5oAuiIKhjONAvgUoQIWudPeyiv63ISfhAA3ymHCgQhDhsk/mki2gAKQWhOM4TKc319m6Npqfx8K10Z2F7RANkG8BKoYD6V2pcHXaUkSCFRyA0kHUYlUCax4WANeCDB94DvDyOCWO3EJoJuUBE6BuHrdIYkxcgPTRDBLbgaoEvyaEylrwixwoakELB7CmYz9+n736AIkLULT6XICy1WfmQHL9zc8LFW5/FkB2/cUPPH0ou/6ERC9AVBQFUL3+FED5+hMA9evfYy6ECX+HJqoWBt3agQ4G3NqBzQbwHkoklekAEyH0NzKhHFzIzgEzAnxHFvl8t6GsbAfMegCyBQK0GIbHjR3oYsBtHWhjwG0d6CPglfTgWhh0Uwca6QHIVg0Aw11NDQCDOgAEPLWYqAcgWw9Atm4KEPFkjdOM96YONJL+nfoiuqsDtSyYyH5QXRwAEfpsCgAgdHFgDGDk2LApwD6tTLC9Ux+uK4H/O/UqsYu6ELR71OCzUnm/1Gv1US3hhqkVdK5Ykd/I9KXVGMhk9T8d3Woc+K93ghLjgCVci4wDQr0RV9iaxNRfmHf2yFbTnT1+TPPeGGPIn2HauHn8GDoPIn1z2ZYhQS/b/LdlOOSnWOS0q4FBxkmX16YA5jO08twUQHqoi/zeqTeeoNW2V9KjCLxfSQ/vdPe90U0ev+nvD8CqOaJJlhYPGuIAPjNPUUi0NiqcpoVYJlzcLbPW8moc39XpA/26RZ2W15eSAWyJIJtPTR34gXYFULalyYKeDrwhiwB2daIWCzwdCJtrvBdUI4QMI7wEYOMwrDfP0YGoCDqXUyOEJPQfRwoAakzkPkklNzR4KxkjSHv3wN8g6evschvdiABGNsPCJinAgBgC+qBlEQqAYX4HWF3mQjqA7wsGMoAG6wHGiWFdQEBcmQBGdlIPO8DIZnAAGKmLdlXmQmo9ANl6ALL1AGTrAchWHQDlaF4HQKkHIFvtAXym066SZTPw13H7qH0ItQf4B6fyltGtFF6SAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=L size=192x197>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with Image.open(image_path) as img:\n",
    "    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d46b3f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a86c1b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images:  2177\n"
     ]
    }
   ],
   "source": [
    "path = '/Users/karthik/GANS/1by4-TOPO-Resized/'\n",
    "\n",
    "img_names = []\n",
    "\n",
    "for folder, subfolders, filenames in os.walk(path):\n",
    "    for img in filenames:\n",
    "        img_names.append(folder+ '/' +img)\n",
    "        \n",
    "print('Images: ',len(img_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "253692bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sizes = []\n",
    "\n",
    "rejected = []\n",
    "\n",
    "for item in img_names:\n",
    "    try:\n",
    "        with Image.open(item) as img:\n",
    "            img_sizes.append(img.size)\n",
    "            \n",
    "    except:\n",
    "        rejected.append(item) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609b51e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "389f1e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2176\n"
     ]
    }
   ],
   "source": [
    "print(len(img_sizes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020850c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e23fc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(img_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b39e7d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>188</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>184</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>192</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>198</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1\n",
       "0  188  179\n",
       "1  184  188\n",
       "2  192  192\n",
       "3  192  192\n",
       "4  198  195"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375fbbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f57bbe0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2176.000000</td>\n",
       "      <td>2176.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>190.460018</td>\n",
       "      <td>190.635110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>5.313203</td>\n",
       "      <td>5.348639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>166.000000</td>\n",
       "      <td>160.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>188.000000</td>\n",
       "      <td>189.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>192.000000</td>\n",
       "      <td>192.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>193.000000</td>\n",
       "      <td>193.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1\n",
       "count  2176.000000  2176.000000\n",
       "mean    190.460018   190.635110\n",
       "std       5.313203     5.348639\n",
       "min     166.000000   160.000000\n",
       "25%     188.000000   189.000000\n",
       "50%     192.000000   192.000000\n",
       "75%     193.000000   193.000000\n",
       "max     200.000000   200.000000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5525753c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b4d44d51",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "773df389",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "channels = 1  \n",
    "height = 192\n",
    "width = 192\n",
    "material_dim = channels * height * width  # 1 * 224 * 224\n",
    "\n",
    "lr = 3e-4\n",
    "z_dim = 112\n",
    "batch_size = 32\n",
    "num_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9dee10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6291532d",
   "metadata": {},
   "source": [
    "**Transformation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354101e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((192,192)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429d8775",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0284013",
   "metadata": {},
   "source": [
    "**Loading the Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74eeb6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "root = '/Users/karthik/GANS/1by4-TOPO-Resized/'\n",
    "print(os.path.exists(root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "89afbfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "class_dir = os.path.join(root, 'bio_materials')\n",
    "os.makedirs(class_dir, exist_ok=True)\n",
    "\n",
    "for entry in os.scandir(root):\n",
    "    if entry.is_file() and entry.name.endswith('.png'):  \n",
    "        shutil.move(entry.path, class_dir)\n",
    "\n",
    "\n",
    "train_data = datasets.ImageFolder(root, transform=transforms)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8d85d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "691f9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "for images, labels in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e0482dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 1, 192, 192])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f14ca6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "511a46ae",
   "metadata": {},
   "source": [
    "**Discriminator Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "703e3ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, d_input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_input_dim, 1024)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features//2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features//2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, 1)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        x = F.dropout(x, 0.3)\n",
    "        return torch.sigmoid(self.fc4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "019d2fae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ac967986",
   "metadata": {},
   "source": [
    "**Generator Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ff512910",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, g_input_dim, g_output_dim):\n",
    "        super(Generator, self).__init__()       \n",
    "        self.fc1 = nn.Linear(g_input_dim, 256)\n",
    "        self.fc2 = nn.Linear(self.fc1.out_features, self.fc1.out_features*2)\n",
    "        self.fc3 = nn.Linear(self.fc2.out_features, self.fc2.out_features*2)\n",
    "        self.fc4 = nn.Linear(self.fc3.out_features, g_output_dim)\n",
    "    \n",
    "    # forward method\n",
    "    def forward(self, x): \n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
    "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
    "        return torch.tanh(self.fc4(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7630b42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29b0717a",
   "metadata": {},
   "source": [
    "**Setting Dimensions for Generator & Discriminator**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ab5bbaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(g_input_dim = z_dim, g_output_dim = material_dim).to(device)\n",
    "disc = Discriminator(d_input_dim = material_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c438ccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5c11fd5",
   "metadata": {},
   "source": [
    "**Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfddea85",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_disc = optim.Adam(disc.parameters(), lr=lr)\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95741f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc50c04a",
   "metadata": {},
   "source": [
    "**Setting the Tensorboard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf9bfb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0\n",
    "fixed_noise = torch.randn(16, z_dim).to(device) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996868e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0017eebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1e09afc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200] Completed - Loss D: 0.2061, loss G: 5.0111\n",
      "Epoch [4/200] Completed - Loss D: 0.4954, loss G: 1.7344\n",
      "Epoch [9/200] Completed - Loss D: 0.2717, loss G: 2.7617\n",
      "Epoch [14/200] Completed - Loss D: 0.4174, loss G: 3.8884\n",
      "Epoch [19/200] Completed - Loss D: 0.1502, loss G: 2.3208\n",
      "Epoch [24/200] Completed - Loss D: 0.2753, loss G: 2.1326\n",
      "Epoch [29/200] Completed - Loss D: 0.4168, loss G: 0.9086\n",
      "Epoch [34/200] Completed - Loss D: 0.2054, loss G: 1.3842\n",
      "Epoch [39/200] Completed - Loss D: 0.3202, loss G: 1.3475\n",
      "Epoch [44/200] Completed - Loss D: 0.3346, loss G: 1.5896\n",
      "Epoch [49/200] Completed - Loss D: 0.2233, loss G: 1.5926\n",
      "Epoch [54/200] Completed - Loss D: 0.2618, loss G: 1.8405\n",
      "Epoch [59/200] Completed - Loss D: 0.1796, loss G: 1.9197\n",
      "Epoch [64/200] Completed - Loss D: 0.3544, loss G: 1.3611\n",
      "Epoch [69/200] Completed - Loss D: 0.2313, loss G: 2.1878\n",
      "Epoch [74/200] Completed - Loss D: 0.5136, loss G: 2.3963\n",
      "Epoch [79/200] Completed - Loss D: 0.4181, loss G: 1.9868\n",
      "Epoch [84/200] Completed - Loss D: 0.4140, loss G: 2.2510\n",
      "Epoch [89/200] Completed - Loss D: 0.3927, loss G: 1.9551\n",
      "Epoch [94/200] Completed - Loss D: 0.3715, loss G: 1.7879\n",
      "Epoch [99/200] Completed - Loss D: 0.3345, loss G: 1.8086\n",
      "Epoch [104/200] Completed - Loss D: 0.3815, loss G: 2.3389\n",
      "Epoch [109/200] Completed - Loss D: 0.4028, loss G: 2.7914\n",
      "Epoch [114/200] Completed - Loss D: 0.3533, loss G: 2.5212\n",
      "Epoch [119/200] Completed - Loss D: 0.1961, loss G: 2.6868\n",
      "Epoch [124/200] Completed - Loss D: 0.4664, loss G: 2.5890\n",
      "Epoch [129/200] Completed - Loss D: 0.5428, loss G: 1.7346\n",
      "Epoch [134/200] Completed - Loss D: 0.5574, loss G: 1.8625\n",
      "Epoch [139/200] Completed - Loss D: 0.2888, loss G: 2.1913\n",
      "Epoch [144/200] Completed - Loss D: 0.3509, loss G: 2.0614\n",
      "Epoch [149/200] Completed - Loss D: 0.3096, loss G: 1.8659\n",
      "Epoch [154/200] Completed - Loss D: 0.5229, loss G: 1.9895\n",
      "Epoch [159/200] Completed - Loss D: 0.4328, loss G: 1.8549\n",
      "Epoch [164/200] Completed - Loss D: 0.3623, loss G: 2.1820\n",
      "Epoch [169/200] Completed - Loss D: 0.3089, loss G: 2.0966\n",
      "Epoch [174/200] Completed - Loss D: 0.4541, loss G: 2.2720\n",
      "Epoch [179/200] Completed - Loss D: 0.3570, loss G: 2.0446\n",
      "Epoch [184/200] Completed - Loss D: 0.3770, loss G: 2.3541\n",
      "Epoch [189/200] Completed - Loss D: 0.4824, loss G: 1.8097\n",
      "Epoch [194/200] Completed - Loss D: 0.4885, loss G: 3.5140\n",
      "Epoch [199/200] Completed - Loss D: 0.4151, loss G: 2.5718\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs): \n",
    "    for batch_idx, (real, _) in enumerate(train_loader):\n",
    "        \n",
    "        real = real.view(-1, material_dim).to(device)  # Flatten the images into vectors\n",
    "        batch_size = real.shape[0]\n",
    "\n",
    "        # Generate random noise as input for the generator\n",
    "        noise = torch.randn(batch_size, z_dim).to(device)\n",
    "        \n",
    "        # Generate fake images with the generator\n",
    "        fake = gen(noise)\n",
    "        \n",
    "        # Compute the discriminator losses on real images\n",
    "        disc_real = disc(real).view(-1)\n",
    "        lossD_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        \n",
    "        # Compute the discriminator losses on fake images\n",
    "        disc_fake = disc(fake).view(-1)\n",
    "        lossD_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        \n",
    "        # Combine losses for the discriminator\n",
    "        lossD = (lossD_real + lossD_fake) / 2\n",
    "        \n",
    "        # Update discriminator\n",
    "        disc.zero_grad()\n",
    "        lossD.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        # Update generator\n",
    "        output = disc(fake).view(-1)\n",
    "        lossG = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        lossG.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "\n",
    "\n",
    "    # After all batches are processed for the current epoch, check if it's time to print\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        with torch.no_grad():\n",
    "            fake = gen(fixed_noise).reshape(-1,1,192,192)\n",
    "            data = real.reshape(-1, 1, 192, 192)\n",
    "            \n",
    "            img_grid_real = torchvision.utils.make_grid(data[:16], normalize=True)\n",
    "            img_grid_fake = torchvision.utils.make_grid(fake[:16], normalize=True)\n",
    "\n",
    "            writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "            writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] Completed - Loss D: {lossD:.4f}, loss G: {lossG:.4f}\")\n",
    "\n",
    "    # Increment the step after each epoch\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63e30b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
