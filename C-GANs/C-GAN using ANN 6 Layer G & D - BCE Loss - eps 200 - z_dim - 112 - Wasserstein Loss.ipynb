{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "263017f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f704ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fabe343",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_csv, transform=None):\n",
    "        \n",
    "\n",
    "        self.labels_df = pd.read_csv(labels_csv)\n",
    "\n",
    "        self.label_map = {row[\"FeatID\"]: row[\"class\"] for index, row in self.labels_df.iterrows()}\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.label_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        feat_id = list(self.label_map.keys())[idx]\n",
    "        img_name = f\"featID {feat_id}.png\"\n",
    "        img_path = os.path.join(self.image_dir, img_name)\n",
    "        image = Image.open(img_path)\n",
    "        label = self.label_map[feat_id]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad7e69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56d03eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_directory = '/Users/karthik/GANS/4by4-TOPO-24X224/renamed_images'\n",
    "labels_csv = '/Users/karthik/AeruginosaWithClass.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c31bf8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8945c",
   "metadata": {},
   "source": [
    "**Parameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c31f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 250\n",
    "\n",
    "image_dim = 1*224*224\n",
    "\n",
    "n_class = 5 \n",
    "\n",
    "latent_dim = 112\n",
    "\n",
    "lr = 0.0002\n",
    "\n",
    "b1 = 0.5\n",
    "\n",
    "b2 = 0.999\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cefd69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e179801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009f1494",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ab20102",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(image_directory, labels_csv, transform=transform)\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for images, labels in data_loader:\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000ed200",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "709e6bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total images: 2100\n",
      "Total labels: 5\n"
     ]
    }
   ],
   "source": [
    "total_images = len(dataset)\n",
    "total_labels = len(set(dataset.labels_df['class']))  \n",
    "\n",
    "print(f\"Total images: {total_images}\")\n",
    "print(f\"Total labels: {total_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c073582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca7af0a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batches: 66\n"
     ]
    }
   ],
   "source": [
    "total_batches = len(data_loader)\n",
    "print(f\"Total batches: {total_batches}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01b5059",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa054d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 1 has 32 images and 32 labels\n"
     ]
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(data_loader, 1):\n",
    "    print(f\"Batch {i} has {len(images)} images and {len(labels)} labels\")\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fe8823",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8098be2b",
   "metadata": {},
   "source": [
    "**Creating a Loop over the data loader to get the labels in the first batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dcd38151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels in the first batch:\n",
      "tensor([0, 0, 4, 1, 0, 1, 0, 3, 4, 0, 2, 3, 2, 1, 4, 0, 1, 1, 2, 4, 2, 2, 2, 1,\n",
      "        2, 2, 3, 2, 0, 2, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "first_batch_labels = next(iter(data_loader))[1] \n",
    "\n",
    "print(\"Labels in the first batch:\")\n",
    "print(first_batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d222f003",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb80d7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e3aab5d",
   "metadata": {},
   "source": [
    "**Generator Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "425ed754",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.label_emb = nn.Embedding(n_class, n_class)\n",
    "\n",
    "        # Define the fully connected layers\n",
    "        self.fc1 = nn.Linear(latent_dim + n_class, 256)\n",
    "        self.fc2 = nn.Linear(256, 512)\n",
    "        self.fc3 = nn.Linear(512, 1024)\n",
    "        self.fc4 = nn.Linear(1024, image_dim)\n",
    "\n",
    "    def forward(self, noise, labels):\n",
    "        \n",
    "        # Concatenate label embedding and image to produce input\n",
    "        x = torch.cat((self.label_emb(labels), noise), -1)\n",
    "        x = F.leaky_relu(self.fc1(x), 0.2)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        x = F.leaky_relu(x, 0.2)\n",
    "\n",
    "        x = self.fc4(x)\n",
    "        img = torch.tanh(x)\n",
    "        \n",
    "        img = img.view(img.size(0), image_dim)\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0a6a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e0cf120",
   "metadata": {},
   "source": [
    "**Discriminator Architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f0124dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        self.label_embedding = nn.Embedding(n_class, n_class)\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(n_class + image_dim, 1024),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(1024, 512),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(512, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            \n",
    "            nn.Linear(256, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, img, labels):\n",
    "        \n",
    "        # Concatenate label embedding and image to produce input\n",
    "        \n",
    "        d_in = torch.cat((img.view(img.size(0), -1), self.label_embedding(labels)), dim= -1)\n",
    "        validity = self.model(d_in)\n",
    "        return validity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2834396a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e48f1af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4161bbeb",
   "metadata": {},
   "source": [
    "**Optiminser**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6206a672",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "discriminator = Discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91894577",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd241f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ca46ce5",
   "metadata": {},
   "source": [
    "**Tensorboard Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e554805b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter \n",
    "\n",
    "writer_fake = SummaryWriter(f\"logs/fake\")\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc6969",
   "metadata": {},
   "source": [
    "**Training Loop**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85217c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(16, latent_dim, device=device)  \n",
    "fixed_labels = torch.randint(0, n_class, (16,), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6f3669bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "FloatTensor = torch.FloatTensor\n",
    "LongTensor = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abaef1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/250] [Batch 65/66] [D loss: -205.914978] [G loss: 0.047701]\n",
      "[Epoch 1/250] [Batch 65/66] [D loss: -7373.247070] [G loss: 8189.881348]\n",
      "[Epoch 2/250] [Batch 65/66] [D loss: 439.121826] [G loss: -1775.374268]\n",
      "[Epoch 3/250] [Batch 65/66] [D loss: 38.551453] [G loss: -693.169983]\n",
      "[Epoch 4/250] [Batch 65/66] [D loss: -204.394043] [G loss: -549.063965]\n",
      "[Epoch 5/250] [Batch 65/66] [D loss: 426.353851] [G loss: -789.808411]\n",
      "[Epoch 6/250] [Batch 65/66] [D loss: -1658.568970] [G loss: 26.124386]\n",
      "[Epoch 7/250] [Batch 65/66] [D loss: 61.130527] [G loss: -61.156097]\n",
      "[Epoch 8/250] [Batch 65/66] [D loss: -758.360291] [G loss: -474.166077]\n",
      "[Epoch 9/250] [Batch 65/66] [D loss: 147.612396] [G loss: -661.038818]\n",
      "[Epoch 10/250] [Batch 65/66] [D loss: 29.463501] [G loss: -263.742462]\n",
      "[Epoch 11/250] [Batch 65/66] [D loss: -142.820770] [G loss: -370.427277]\n",
      "[Epoch 12/250] [Batch 65/66] [D loss: 50.995667] [G loss: -540.826965]\n",
      "[Epoch 13/250] [Batch 65/66] [D loss: 314.013916] [G loss: -964.511597]\n",
      "[Epoch 14/250] [Batch 65/66] [D loss: 111.465149] [G loss: -335.394257]\n",
      "[Epoch 15/250] [Batch 65/66] [D loss: -240.333496] [G loss: -148.629181]\n",
      "[Epoch 16/250] [Batch 65/66] [D loss: -51.110352] [G loss: 1098.446777]\n",
      "[Epoch 17/250] [Batch 65/66] [D loss: -166.946533] [G loss: -116.006973]\n",
      "[Epoch 18/250] [Batch 65/66] [D loss: -765.315979] [G loss: 1421.645508]\n",
      "[Epoch 19/250] [Batch 65/66] [D loss: -312.045715] [G loss: -739.141968]\n",
      "[Epoch 20/250] [Batch 65/66] [D loss: 163.637131] [G loss: -3.597861]\n",
      "[Epoch 21/250] [Batch 65/66] [D loss: -347.592560] [G loss: -262.977142]\n",
      "[Epoch 22/250] [Batch 65/66] [D loss: -170.618210] [G loss: -81.152176]\n",
      "[Epoch 23/250] [Batch 65/66] [D loss: 29.725121] [G loss: -4.999784]\n",
      "[Epoch 24/250] [Batch 65/66] [D loss: -2.718459] [G loss: 2.866879]\n",
      "[Epoch 25/250] [Batch 65/66] [D loss: -638.216675] [G loss: -151.111908]\n",
      "[Epoch 26/250] [Batch 65/66] [D loss: 25.581450] [G loss: 2.571085]\n",
      "[Epoch 27/250] [Batch 65/66] [D loss: -833.370850] [G loss: 1394.270020]\n",
      "[Epoch 28/250] [Batch 65/66] [D loss: 8.298816] [G loss: -2.888249]\n",
      "[Epoch 29/250] [Batch 65/66] [D loss: 461.540833] [G loss: -792.819153]\n",
      "[Epoch 30/250] [Batch 65/66] [D loss: -124.929642] [G loss: 199.876862]\n",
      "[Epoch 31/250] [Batch 65/66] [D loss: -54.940014] [G loss: -37.249866]\n",
      "[Epoch 32/250] [Batch 65/66] [D loss: -93.292877] [G loss: 338.607483]\n",
      "[Epoch 33/250] [Batch 65/66] [D loss: -355.655151] [G loss: 7.424090]\n",
      "[Epoch 34/250] [Batch 65/66] [D loss: -22.926497] [G loss: -17.907032]\n",
      "[Epoch 35/250] [Batch 65/66] [D loss: -25.520573] [G loss: 27.651897]\n",
      "[Epoch 36/250] [Batch 65/66] [D loss: -126.007515] [G loss: 7.155844]\n",
      "[Epoch 37/250] [Batch 65/66] [D loss: 182.149811] [G loss: 94.463341]\n",
      "[Epoch 38/250] [Batch 65/66] [D loss: 8.081589] [G loss: -165.143677]\n",
      "[Epoch 39/250] [Batch 65/66] [D loss: -18.735964] [G loss: -19.016912]\n",
      "[Epoch 40/250] [Batch 65/66] [D loss: -68.850708] [G loss: 0.840802]\n",
      "[Epoch 41/250] [Batch 65/66] [D loss: -49.576656] [G loss: 5.077348]\n",
      "[Epoch 42/250] [Batch 65/66] [D loss: -912.283691] [G loss: -517.528931]\n",
      "[Epoch 43/250] [Batch 65/66] [D loss: 4.303279] [G loss: -7.064130]\n",
      "[Epoch 44/250] [Batch 65/66] [D loss: -141.982590] [G loss: 187.874039]\n",
      "[Epoch 45/250] [Batch 65/66] [D loss: -369.381592] [G loss: -1834.190186]\n",
      "[Epoch 46/250] [Batch 65/66] [D loss: -1097.458252] [G loss: 3294.406250]\n",
      "[Epoch 47/250] [Batch 65/66] [D loss: -193.534988] [G loss: 269.904785]\n",
      "[Epoch 48/250] [Batch 65/66] [D loss: -1106.137573] [G loss: -1895.286499]\n",
      "[Epoch 49/250] [Batch 65/66] [D loss: -215.904434] [G loss: -155.924408]\n",
      "[Epoch 50/250] [Batch 65/66] [D loss: 234.665253] [G loss: -16.549435]\n",
      "[Epoch 51/250] [Batch 65/66] [D loss: 2047.819580] [G loss: 2579.578369]\n",
      "[Epoch 52/250] [Batch 65/66] [D loss: 1009.992676] [G loss: -3447.626221]\n",
      "[Epoch 53/250] [Batch 65/66] [D loss: 26.350449] [G loss: -6.979231]\n",
      "[Epoch 54/250] [Batch 65/66] [D loss: 531.076172] [G loss: -3798.816406]\n",
      "[Epoch 55/250] [Batch 65/66] [D loss: -252.392365] [G loss: 335.028473]\n",
      "[Epoch 56/250] [Batch 65/66] [D loss: -113.630638] [G loss: -71.137268]\n",
      "[Epoch 57/250] [Batch 65/66] [D loss: 249.668610] [G loss: -529.731018]\n",
      "[Epoch 58/250] [Batch 65/66] [D loss: -125.838707] [G loss: -42.301727]\n",
      "[Epoch 59/250] [Batch 65/66] [D loss: 174.563507] [G loss: -411.660065]\n",
      "[Epoch 60/250] [Batch 65/66] [D loss: -198.089279] [G loss: -114.267555]\n",
      "[Epoch 61/250] [Batch 65/66] [D loss: -65.817131] [G loss: 24.360214]\n",
      "[Epoch 62/250] [Batch 65/66] [D loss: -365.576782] [G loss: -353.372375]\n",
      "[Epoch 63/250] [Batch 65/66] [D loss: -180.202271] [G loss: 230.253784]\n",
      "[Epoch 64/250] [Batch 65/66] [D loss: -406.315674] [G loss: -174.499420]\n",
      "[Epoch 65/250] [Batch 65/66] [D loss: -1135.438599] [G loss: 1875.239624]\n",
      "[Epoch 66/250] [Batch 65/66] [D loss: -1114.734009] [G loss: -816.869141]\n",
      "[Epoch 67/250] [Batch 65/66] [D loss: -1260.661621] [G loss: 4032.406250]\n",
      "[Epoch 68/250] [Batch 65/66] [D loss: -770.629395] [G loss: -952.375488]\n",
      "[Epoch 69/250] [Batch 65/66] [D loss: -797.483887] [G loss: 5469.735840]\n",
      "[Epoch 70/250] [Batch 65/66] [D loss: -564.555603] [G loss: -695.116638]\n",
      "[Epoch 71/250] [Batch 65/66] [D loss: -708.526611] [G loss: 1517.736694]\n",
      "[Epoch 72/250] [Batch 65/66] [D loss: -210.986191] [G loss: -202.986740]\n",
      "[Epoch 73/250] [Batch 65/66] [D loss: 89.977890] [G loss: -270.996338]\n",
      "[Epoch 74/250] [Batch 65/66] [D loss: -256.257263] [G loss: 35.641521]\n",
      "[Epoch 75/250] [Batch 65/66] [D loss: -1035.613770] [G loss: 4769.184570]\n",
      "[Epoch 76/250] [Batch 65/66] [D loss: 261.160065] [G loss: -687.133972]\n",
      "[Epoch 77/250] [Batch 65/66] [D loss: -213.564941] [G loss: -2367.029785]\n",
      "[Epoch 78/250] [Batch 65/66] [D loss: -201.424301] [G loss: -189.216888]\n",
      "[Epoch 79/250] [Batch 65/66] [D loss: -316.304077] [G loss: 769.711060]\n",
      "[Epoch 80/250] [Batch 65/66] [D loss: -1072.947510] [G loss: -1797.231812]\n",
      "[Epoch 81/250] [Batch 65/66] [D loss: 1196.605225] [G loss: 3109.478271]\n",
      "[Epoch 82/250] [Batch 65/66] [D loss: -912.680420] [G loss: -1524.073364]\n",
      "[Epoch 83/250] [Batch 65/66] [D loss: -2120.397461] [G loss: 6502.520508]\n",
      "[Epoch 84/250] [Batch 65/66] [D loss: -440.773682] [G loss: -2295.069824]\n",
      "[Epoch 85/250] [Batch 65/66] [D loss: -1118.635742] [G loss: 5839.662109]\n",
      "[Epoch 86/250] [Batch 65/66] [D loss: -337.030029] [G loss: -591.622742]\n",
      "[Epoch 87/250] [Batch 65/66] [D loss: -391.100464] [G loss: 698.704712]\n",
      "[Epoch 88/250] [Batch 65/66] [D loss: -75.889740] [G loss: -53.088066]\n",
      "[Epoch 89/250] [Batch 65/66] [D loss: 465.514221] [G loss: -1530.518799]\n",
      "[Epoch 90/250] [Batch 65/66] [D loss: -1065.929199] [G loss: -2209.778809]\n",
      "[Epoch 91/250] [Batch 65/66] [D loss: 1138.619141] [G loss: 2857.265137]\n",
      "[Epoch 92/250] [Batch 65/66] [D loss: -651.898926] [G loss: -2872.378418]\n",
      "[Epoch 93/250] [Batch 65/66] [D loss: 1826.373169] [G loss: 1266.940674]\n",
      "[Epoch 94/250] [Batch 65/66] [D loss: 162.591400] [G loss: -592.576050]\n",
      "[Epoch 95/250] [Batch 65/66] [D loss: -145.788208] [G loss: -74.288124]\n",
      "[Epoch 96/250] [Batch 65/66] [D loss: 38.569107] [G loss: -105.857567]\n",
      "[Epoch 97/250] [Batch 65/66] [D loss: -270.530060] [G loss: -59.034264]\n",
      "[Epoch 98/250] [Batch 65/66] [D loss: -661.247559] [G loss: 1159.587769]\n",
      "[Epoch 99/250] [Batch 65/66] [D loss: -106.519424] [G loss: -34.378532]\n",
      "[Epoch 100/250] [Batch 65/66] [D loss: 900.492432] [G loss: -1649.525024]\n",
      "[Epoch 101/250] [Batch 65/66] [D loss: 1429.653076] [G loss: 1010.433899]\n",
      "[Epoch 102/250] [Batch 65/66] [D loss: 126.888184] [G loss: -5638.163086]\n",
      "[Epoch 103/250] [Batch 65/66] [D loss: 1774.256714] [G loss: 1501.244019]\n",
      "[Epoch 104/250] [Batch 65/66] [D loss: 536.692627] [G loss: -3855.114014]\n",
      "[Epoch 105/250] [Batch 65/66] [D loss: -256.313049] [G loss: -73.268356]\n",
      "[Epoch 106/250] [Batch 65/66] [D loss: -1023.923584] [G loss: 1955.219116]\n",
      "[Epoch 107/250] [Batch 65/66] [D loss: -917.209717] [G loss: -2100.770752]\n",
      "[Epoch 108/250] [Batch 65/66] [D loss: 919.079102] [G loss: 8098.210938]\n",
      "[Epoch 109/250] [Batch 65/66] [D loss: -1045.749268] [G loss: -3028.142090]\n",
      "[Epoch 110/250] [Batch 65/66] [D loss: -1198.591797] [G loss: 4578.997070]\n",
      "[Epoch 111/250] [Batch 65/66] [D loss: -652.428894] [G loss: -887.640747]\n",
      "[Epoch 112/250] [Batch 65/66] [D loss: -195.917786] [G loss: 510.535706]\n",
      "[Epoch 113/250] [Batch 65/66] [D loss: -79.967873] [G loss: 3.375291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 114/250] [Batch 65/66] [D loss: 661.677490] [G loss: -3481.932861]\n",
      "[Epoch 115/250] [Batch 65/66] [D loss: -693.552490] [G loss: -1360.085205]\n",
      "[Epoch 116/250] [Batch 65/66] [D loss: -220.800293] [G loss: 6260.166504]\n",
      "[Epoch 117/250] [Batch 65/66] [D loss: 1159.397217] [G loss: -3024.096436]\n",
      "[Epoch 118/250] [Batch 65/66] [D loss: -328.776184] [G loss: -347.349457]\n",
      "[Epoch 119/250] [Batch 65/66] [D loss: -105.953247] [G loss: 461.535736]\n",
      "[Epoch 120/250] [Batch 65/66] [D loss: -328.072113] [G loss: -252.373749]\n",
      "[Epoch 121/250] [Batch 65/66] [D loss: 81.658173] [G loss: -157.668915]\n",
      "[Epoch 122/250] [Batch 65/66] [D loss: 186.249146] [G loss: 220.450043]\n",
      "[Epoch 123/250] [Batch 65/66] [D loss: 189.932617] [G loss: -5528.897461]\n",
      "[Epoch 124/250] [Batch 65/66] [D loss: -1251.512207] [G loss: 6844.947754]\n",
      "[Epoch 125/250] [Batch 65/66] [D loss: -2102.646729] [G loss: -1549.583252]\n",
      "[Epoch 126/250] [Batch 65/66] [D loss: -1454.045898] [G loss: 5585.660156]\n",
      "[Epoch 127/250] [Batch 65/66] [D loss: -302.747833] [G loss: -289.264221]\n",
      "[Epoch 128/250] [Batch 65/66] [D loss: 198.341003] [G loss: -524.250854]\n",
      "[Epoch 129/250] [Batch 65/66] [D loss: 341.177856] [G loss: 185.909790]\n",
      "[Epoch 130/250] [Batch 65/66] [D loss: 771.205566] [G loss: -5016.301758]\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    for i, (imgs, labels) in enumerate(data_loader):\n",
    "        \n",
    "        batch_size = imgs.shape[0]\n",
    "\n",
    "        real_imgs = imgs.type(FloatTensor)\n",
    "        labels = labels.type(LongTensor)\n",
    "\n",
    "        # Training Generator\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # Sample noise and labels as generator input\n",
    "        z = FloatTensor(np.random.normal(0, 1, (batch_size, latent_dim)))\n",
    "        gen_labels = LongTensor(np.random.randint(0, n_class, batch_size))\n",
    "\n",
    "        # Generate a batch of images\n",
    "        gen_imgs = generator(z, gen_labels)\n",
    "        \n",
    "\n",
    "        # Loss measures generator's ability to fool the discriminator\n",
    "        # The negative sign is used for gradient ascent\n",
    "        g_loss = -torch.mean(discriminator(gen_imgs, gen_labels))\n",
    "\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Training Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Measure discriminator's ability to classify real from generated samples\n",
    "        real_loss = torch.mean(discriminator(real_imgs, labels))\n",
    "        fake_loss = torch.mean(discriminator(gen_imgs.detach(), gen_labels))\n",
    "        d_loss = fake_loss - real_loss\n",
    "\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "        \n",
    "        for p in discriminator.parameters():\n",
    "            p.data.clamp_(-0.01, 0.01)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    if epoch % 1 == 0:\n",
    "\n",
    "        print(\n",
    "                \"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\"\n",
    "                % (epoch, n_epochs, i, len(data_loader), d_loss.item(), g_loss.item())\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e64506a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "710f31c1",
   "metadata": {},
   "source": [
    "**To Generate Specific Sample Images**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8165ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Define the specific label and the number of images you want to generate\n",
    "specific_label = 1\n",
    "num_images = 16  # Example: generate 5 images\n",
    "\n",
    "# Assuming you're using a GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Generate noise vectors\n",
    "noise = torch.randn(num_images, latent_dim).to(device)\n",
    "\n",
    "# Create label tensor (all elements are the specific label)\n",
    "labels = torch.full((num_images,), specific_label, dtype=torch.long).to(device)\n",
    "\n",
    "# Generate images\n",
    "with torch.no_grad():\n",
    "    generated_images = generator(noise, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3b4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbd79ad3",
   "metadata": {},
   "source": [
    "**To Visualise the Generated Image**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d1ab0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming generated_images is a batch of images\n",
    "num_images = 16\n",
    "rows = 4\n",
    "cols = 4  # 4x4 grid for 16 images\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "\n",
    "for i, img in enumerate(generated_images):\n",
    "    ax = axes[i // cols, i % cols]  # Determine the row and column\n",
    "\n",
    "    # Reshape the image from (50176,) to (224, 224) and normalize it to [0, 1]\n",
    "    img = img.view(224, 224)  # Reshape\n",
    "    img = (img + 1) / 2       # Normalize\n",
    "    img = img.clamp(0, 1)\n",
    "    img = img.cpu()\n",
    "\n",
    "    # Display the image\n",
    "    ax.imshow(img.numpy(), cmap='gray')\n",
    "    ax.axis('off')  # Hide axes\n",
    "\n",
    "# Ensure there are no empty plots if there are fewer than 16 images\n",
    "for i in range(len(generated_images), rows * cols):\n",
    "    axes[i // cols, i % cols].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357e435d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
